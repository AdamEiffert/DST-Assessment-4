{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bdb672f",
   "metadata": {},
   "source": [
    "# Neural Network Training\n",
    "We will now use the training and testing datasets to train the nerual networks. We will once again look at the code for approach 3 here. For the HPC code, please look at 03-hpc.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82cda48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e8059ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30e00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../data/processed/test2.csv\").drop(columns = ['Unnamed: 0'])\n",
    "train = pd.read_csv(\"../data/processed/train2.csv\").drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3f3d6d",
   "metadata": {},
   "source": [
    "Features is the number of descriptive features in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0415e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for approach 1 and 2\n",
    "features = train.shape[1] -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fb579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train.shape[1] -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a2650",
   "metadata": {},
   "source": [
    "## Create Models\n",
    "We will now create the models that we will train. As they each follow a very similar pattern, we will use a function to define them. For each model we will use a dense sequential nerual net with either 1, 3, 5 or 10 hidden layers, each with the activation function of tanh, RELU or Swish. The input and hidden layers each have the `features` number of nodes. Then for the output layer we have 5 nodes (one for each of the classifications) and a sigmoid function to allow us to classify the data. \n",
    "\n",
    "We have used binary crossentropy allows us to minimise the error when classifying the attacks and we are using the Adam optimiser as this is a good and commonly optimiser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d08e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(features,layers,activation):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(features, input_dim=features, activation=activation))\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(features, activation=activation))\n",
    "    model.add(Dense(5, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eebcde",
   "metadata": {},
   "source": [
    "We will now use this function to create the different models that we will test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dca66e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-07 20:11:06.257076: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-07 20:11:06.257123: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-07 20:11:06.257152: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c-VirtualBox): /proc/driver/nvidia/version does not exist\n",
      "2022-03-07 20:11:06.257511: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model1_tanh = create_model(features,1,'tanh')\n",
    "model1_relu = create_model(features,1,'relu')\n",
    "model1_swish = create_model(features,1,'swish')\n",
    "model3_tanh = create_model(features,3,'tanh')\n",
    "model3_relu = create_model(features,3,'relu')\n",
    "model3_swish = create_model(features,3,'swish')\n",
    "model5_tanh = create_model(features,5,'tanh')\n",
    "model5_relu = create_model(features,5,'relu')\n",
    "model5_swish = create_model(features,5,'swish')\n",
    "model10_tanh = create_model(features,10,'tanh')\n",
    "model10_relu = create_model(features,10,'relu')\n",
    "model10_swish = create_model(features,10,'swish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a5053",
   "metadata": {},
   "source": [
    "## Train models\n",
    "We will now train the models that we have created. To do this we will train each model and after each epoch we will test the model using the test dataset and then will stop training the model if the model does worse than in the previous epoch. This approach allows us to track the progress of the model throughout the training process, however it does not leave any scope for finding it's way out of a local minima. We decided that this was a worthy trade off as we would be training many models so we would have time constraints, and we are also not neccesarily looking to find a global minima as we are comparing the models under the same circumstances. We used the AUC metric instead of accuracy as this helps gives a more holistic metric describing the confusion metric than accuracy. The reasons that we are saving the model is for the extension of this project. Here we have to predict the model separately due to the different classifications and then calculate the accuracy and AUC using this prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea19da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train,test,model,num):\n",
    "    #separate the test and train datasets \n",
    "    train_X = train.drop(['normal','dos','u2r','r2l','probe'],axis=1)\n",
    "    test_X = train.drop(['normal','dos','u2r','r2l','probe'],axis=1)\n",
    "    train_Y = train.loc[:,['normal','dos','u2r','r2l','probe']]\n",
    "    test_Y = train.loc[:,['normal','dos','u2r','r2l','probe']]\n",
    "    old_score = 0 #used to track the AUC score of the last epoch\n",
    "    #initialise the model metrics\n",
    "    test_score_acc=[]\n",
    "    train_score_acc=[]\n",
    "    test_score_auc=[]\n",
    "    train_score_auc=[]\n",
    "    model_time = []\n",
    "    test_norm_mat=[]\n",
    "    test_dos_mat=[]\n",
    "    test_u2r_mat=[]\n",
    "    test_r2l_mat=[]\n",
    "    test_probe_mat=[]\n",
    "    epoch = 0\n",
    "    while (epoch < 2) or (test_score_auc[epoch-1] > test_score_auc[epoch-2]): #repeat until the test AUC score of the latest iteration is lower than the previous epoch\n",
    "        model_time1 = time.perf_counter() #check the time\n",
    "        model.fit(train_X,train_Y,epochs=1,batch_size=128)\n",
    "        model_time2 = time.perf_counter() #check the time\n",
    "        model_time.append(model_time2 - model_time1) #calculates the difference in the times to see how long the model took to train\n",
    "        yhat = model.predict(train_X) #predicts the classification of the train dataset\n",
    "        yhat = yhat.round() #round to classify the data\n",
    "        train_score_acc.append(accuracy_score(train_Y,yhat)) #calculate accuracy of the prediction\n",
    "        train_score_auc.append(roc_auc_score(train_Y,yhat)) #calculate AUC of the prediction\n",
    "        yhat = model.predict(test_X) #predicts the classification of the test dataset\n",
    "        yhat = yhat.round() #round to classify the data\n",
    "        yhat1=[[] for _ in range(5)] #create a matrix reformat the prediction\n",
    "        #reformat the model prediction so we can separate the attack types\n",
    "        for i in range(len(yhat)): \n",
    "            for j in range(5):\n",
    "                yhat1[j].append(yhat[i][j])\n",
    "        test_score_acc.append(accuracy_score(test_Y,yhat)) #calculate accuracy of the prediction\n",
    "        test_score_auc.append(roc_auc_score(test_Y,yhat)) #calculate AUC of the prediction\n",
    "        #calculate the confusion matrix for each of the classifications\n",
    "        test_norm_mat=confusion_matrix(test_Y['normal'],yhat1[0])\n",
    "        test_dos_mat=confusion_matrix(test_Y['dos'],yhat1[1])\n",
    "        test_u2r_mat=confusion_matrix(test_Y['u2r'],yhat1[2])\n",
    "        test_r2l_mat=confusion_matrix(test_Y['r2l'],yhat1[3])\n",
    "        test_probe_mat=confusion_matrix(test_Y['probe'],yhat1[4])\n",
    "        epoch = epoch + 1\n",
    "    #manipulate the model times to useful metrics\n",
    "    avg_time = statistics.mean(model_time[:-1])\n",
    "    total_time=sum(model_time[:-1])\n",
    "    model.save('model'+str(num)) #save models for the extension\n",
    "    return (test_score_acc[:-1], train_score_acc[:-1], test_score_auc[:-1], train_score_auc[:-1], avg_time, total_time, test_norm_mat[:-1], test_dos_mat[:-1], test_u2r_mat[:-1], test_r2l_mat[:-1], test_probe_mat[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d6cafb",
   "metadata": {},
   "source": [
    "We now need to call the training function on each of the models and then save the metrics so that we can evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1775d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "score=[]\n",
    "score.append(train_model(train,test,model1_tanh,1))\n",
    "score.append(train_model(train,test,model1_relu,2))\n",
    "score.append(train_model(train,test,model1_swish,3))\n",
    "score.append(train_model(train,test,model3_tanh,4))\n",
    "score.append(train_model(train,test,model3_relu,5))\n",
    "score.append(train_model(train,test,model3_swish,6))\n",
    "score.append(train_model(train,test,model5_tanh,7))\n",
    "score.append(train_model(train,test,model5_relu,8))\n",
    "score.append(train_model(train,test,model5_swish,9))\n",
    "score.append(train_model(train,test,model10_tanh,10))\n",
    "score.append(train_model(train,test,model10_relu,11))\n",
    "score.append(train_model(train,test,model10_swish,12))\n",
    "with open('/user/work/zg18997/scores2','wb') as f:\n",
    "\tpickle.dump(score, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
